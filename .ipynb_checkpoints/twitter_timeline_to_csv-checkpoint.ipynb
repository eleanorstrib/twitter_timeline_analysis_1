{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Six Steps to Transforming Any Twitter User's Timeline into a csv or Dictionary with Python\n",
    "\n",
    "###### By Eleanor Stribling, July 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you enjoy Natural Language Processing like I do, you might have some ideas for analyses you could do using Twitter. The goal of this two part project is to show you how to:\n",
    "\n",
    "1. Obtain and clean the data from a Twitter user's timeline\n",
    "\n",
    "2. Export into a csv file and a Python dictonary\n",
    "\n",
    "3. Apply analysis tools from NLTK to get summary data and explore hypotheses about the user's tweets\n",
    "\n",
    "4. Use Python libraries to visualize the data you collect\n",
    "\n",
    "In this workbook, we'll learn how to gather any user's timeline using just Python and a few libraries. Then, we'll get ready to do the fun part of the project - the analysis - by cleaning the data and putting it into your choice of a csv file or Python dictionary.\n",
    "\n",
    "I'm using Python 3.6 in this project, as well as an [Anaconda virtual environment](https://conda.io/docs/user-guide/tasks/manage-environments.html).  To complete the steps below, you should already have Python 3.6 and Anaconda installed and the virtual environment activated. You will also need a Twitter account and an internet connection to complete this workbook. \n",
    "\n",
    "Since I code on a Mac I use some terminology that doesn't translate to a Windows machine in places; terminal instead of shell for example.\n",
    "\n",
    "If you're not too familiar with Jupyter Notebooks, they are awesome, and [this quick start guide](http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/) is well worth your time.  Knowing the basics will help you through this notebook and be a useful tool when you do your own projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install the required packages\n",
    "From your virtual environment, you can run the `requirements.txt` file in this repo, rather than searching out and installing each one separately.  \n",
    "\n",
    "To do this, go to the folder you'll be working from in your terminal, move the `requirements.txt` file over to that folder, and run the command `pip install -r requirements.txt`.\n",
    "\n",
    "Once that's installed, type `jupyter notebook twitter_timeline_to_csv`, and the notebook will open in the browser.\n",
    "\n",
    "Running the cell below will import all of the needed packages into your script and make them ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import twitter\n",
    "import lxml.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Get ready to talk to the Twitter API\n",
    "If you don't already have a Twitter account, you can create one [here](https://twitter.com/i/flow/signup).\n",
    "\n",
    "If you do have a Twitter account, head over to the Twitter API documentation - to get the keys needed to access the API, you'll need to create an application.  Navigate to the [apps page](https://apps.twitter.com) while signed into your account and click on the `Create New App` button on the top right.\n",
    "<img src=\"images/applications_page.png\"/>\n",
    "\n",
    "Once that's done, click on your app's name from the list and \n",
    "\n",
    "Next, you need to go get four things:\n",
    "- Consumer key\n",
    "- Consumer secret key\n",
    "- Access token\n",
    "- Access secret token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_CONS_KEY = os.environ.get('T_CONS_')\n",
    "TWITTER_CONS_SEC = os.environ.get('T_CONS_SECRET')\n",
    "TWITTER_ACCESS_TOKEN = os.environ.get('T_ACCESS_')\n",
    "TWITTER_ACCESS_SEC = os.environ.get('T_ACCESS_SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = twitter.Api(\n",
    "    consumer_key = TWITTER_CONS_KEY,\n",
    "    consumer_secret = TWITTER_CONS_SEC,\n",
    "    access_token_key = TWITTER_ACCESS_TOKEN, \n",
    "    access_token_secret = TWITTER_ACCESS_SEC,\n",
    "    tweet_mode='extended'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you'll need to create a variable for the account that you want to analyze.  Use the Twitter handle on the account, without the '@' (e.g. Use NASA, not @NASA).  If you're starting with the URL of the account, take only the handle after the last '/' (e.g. From https://twitter.com/NASA, use 'NASA'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_name = \"NASA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Twitter api\n",
    "first_200 = t.GetUserTimeline(screen_name=screen_name, count=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(first_200, screen_name, last_id):\n",
    "    all_tweets = []\n",
    "    all_tweets.extend(first_200)\n",
    "    for i in range(900):\n",
    "        new = t.GetUserTimeline(screen_name=screen_name, max_id=last_id-1)\n",
    "        all_tweets.extend(new)\n",
    "        if len(new) > 0:\n",
    "            last_id = new[-1].id\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = get_tweets(first_200, screen_name, first_200[-1].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We collected 3238 tweets.\n",
      "The most recent tweet in our collection was sent Sun Jul 01 01:25:01 +0000 2018 and the oldest tweet was sent Wed Oct 11 15:31:37 +0000 2017.\n"
     ]
    }
   ],
   "source": [
    "print(\"We collected %d tweets.\" % len(all_tweets))\n",
    "print(\"The most recent tweet in our collection was sent %s and the oldest tweet was sent %s.\" % (\n",
    "                                                                            all_tweets[0].created_at, \n",
    "                                                                            all_tweets[-1].created_at)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'created_at' parameter is in the form of a <class 'str'> and looks like this: Sun Jul 01 01:25:01 +0000 2018.\n",
      "The 'created_at' parameter is in the form of a <class 'str'> and looks like this: <a href=\"https://www.sprinklr.com\" rel=\"nofollow\">Sprinklr</a>.\n"
     ]
    }
   ],
   "source": [
    "print(\"The 'created_at' parameter is in the form of a %s and looks like this: %s.\" % (\n",
    "                                        type(all_tweets[0].created_at),\n",
    "                                        all_tweets[0].created_at)\n",
    "     )\n",
    "print(\"The 'created_at' parameter is in the form of a %s and looks like this: %s.\" % (\n",
    "                                        type(all_tweets[0].source), \n",
    "                                        all_tweets[0].source)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtags(hashtags):\n",
    "    cleaned = []\n",
    "    if len(hashtags) >= 1:\n",
    "        for i in range(len(hashtags)):\n",
    "            cleaned.append(hashtags[i].text)        \n",
    "    return cleaned\n",
    "\n",
    "def clean_urls(urls):\n",
    "    cleaned = []\n",
    "    if len(urls) >= 1:\n",
    "        for i in range(len(urls)):\n",
    "            cleaned.append(urls[i].expanded_url)\n",
    "    return(cleaned)\n",
    "        \n",
    "\n",
    "def clean_source(source):\n",
    "    raw = lxml.html.document_fromstring(source)\n",
    "    return raw.cssselect('body')[0].text_content()\n",
    "\n",
    "\n",
    "def string_to_datetime(date_str):\n",
    "    return datetime.strptime(date_str, '%a %b %d %H:%M:%S %z %Y')\n",
    "\n",
    "\n",
    "# print('Our cleaned text; original was \"<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>)\":')\n",
    "# print(clean_source('<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>'))\n",
    "# print('***')\n",
    "# print('Our date string converted to an object; original was \"Sat Jun 30 11:37:03 +0000 2018\":')\n",
    "# print(string_to_datetime('Sat Jun 30 11:37:03 +0000 2018'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(tweets, filename):\n",
    "    headers = ['id', 'full_text', 'hashtags', 'urls', 'created_at', 'favorite_count', 'retweet_count', 'source']\n",
    "            \n",
    "    with open(filename + '.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        for item in tweets:\n",
    "            writer.writerow([item.id, \n",
    "                             item.full_text, \n",
    "                             clean_hashtags(item.hashtags), \n",
    "                             clean_urls(item.urls), \n",
    "                             item.created_at, \n",
    "                             item.favorite_count, \n",
    "                             item.retweet_count, \n",
    "                             clean_source(item.source)])\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_csv(all_tweets, screen_name + '_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(tweets):\n",
    "    dict = {}\n",
    "    for item in tweets:\n",
    "        clean_source(item.source)\n",
    "        dict[str(item.id)] = {\n",
    "            'id':item.id,\n",
    "            'full_text': item.full_text,\n",
    "            'hashtags': item.hashtags,\n",
    "            'urls': item.urls,\n",
    "            'created_at': string_to_datetime(item.created_at),\n",
    "            'favorite_count': item.favorite_count,\n",
    "            'retweet_count' : item.retweet_count,\n",
    "            'source': clean_source(item.source)\n",
    "        }\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict = create_dict(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict['1013023608040513537']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
