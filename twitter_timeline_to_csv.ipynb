{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Five Steps to Transforming Any Twitter User's Timeline into a csv or Dictionary with Python\n",
    "\n",
    "###### By Eleanor Stribling, July 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you enjoy Natural Language Processing like I do, you might have some ideas for analyses you could do using Twitter. What are the word patterns and hashtags that the tweeter uses?  How do these change over time?  What sites do they link to?  What words appear around others?  Who or what do they talk about?\n",
    "\n",
    "This is part 1 of a 2 part project to show you how to:\n",
    "\n",
    "A. Obtain and clean the data from a Twitter user's timeline\n",
    "\n",
    "B. Export into a csv file and a Python dictonary\n",
    "\n",
    "C. Apply analysis tools from NLTK to get summary data and explore hypotheses about the user's tweets\n",
    "\n",
    "D. Use Python libraries to visualize the data you collect\n",
    "\n",
    "In this workbook, we'll cover A and B from the list above.  First, I'll show you how to gather any user's timeline using just Python and a few libraries. Then, we'll get ready to do the fun part of the project - the analysis - by cleaning the data and putting it into your choice of a csv file or Python dictionary.\n",
    "\n",
    "I'm using Python 3.6 in this project, as well as an [Anaconda virtual environment](https://conda.io/docs/user-guide/tasks/manage-environments.html).  To complete the steps below, you should already have Python 3.6 and Anaconda installed and the virtual environment activated. You will also need a Twitter account and an internet connection to complete this workbook. \n",
    "\n",
    "Since I code on a Mac I use some terminology that doesn't translate to a Windows machine in places; terminal instead of shell for example. \n",
    "\n",
    "If you're not too familiar with Jupyter Notebooks, they are awesome, and [this quick start guide](http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/) is well worth your time.  Knowing the basics will help you through this notebook and be a useful tool when you do your own projects!\n",
    "\n",
    "This code lives on Github, and is the subject of [this post](https://medium.com/p/ff41b941ed6) on my technical blog, [agatha.codes](https://medium.com/agatha-codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install the required packages üë©üèª‚Äçüíª\n",
    "From your virtual environment, you can run the `requirements.txt` file in this repo, rather than searching out and installing each one separately.  \n",
    "\n",
    "To do this, go to the folder you'll be working from in your terminal, move the `requirements.txt` file over to that folder, and run the command `pip install -r requirements.txt`.\n",
    "\n",
    "Once that's installed, type `jupyter notebook twitter_timeline_to_csv`, and the notebook will open in the browser.\n",
    "\n",
    "Running the cell below will import all of the needed packages into your script and make them ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import twitter\n",
    "import lxml.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get ready to talk to the Twitter API üìû\n",
    "If you don't already have a Twitter account, you can create one [here](https://twitter.com/i/flow/signup).\n",
    "\n",
    "If you do have a Twitter account, head over to the Twitter API documentation - to get the keys needed to access the API, you'll need to create an application.  Navigate to the [apps page](https://apps.twitter.com) while signed into your account and click on the `Create New App` button on the top right.\n",
    "<img src=\"images/applications_page.png\"/>\n",
    "\n",
    "Once that's done, click on your app's name from the list and then the \"Keys and access tokens\" tab. \n",
    "<img src=\"images/keys.png\"/>\n",
    "\n",
    "Here, you'll need to get four things that will identify you and give you access to the Twitter API:\n",
    "\n",
    "From the top of the page:\n",
    "- Consumer key (API key)\n",
    "- Consumer secret key (API secret key)\n",
    "\n",
    "<img src=\"images/consumer_api.png\" />\n",
    "\n",
    "From the bottom of the page:\n",
    "- Access token\n",
    "- Access secret token\n",
    "\n",
    "<img src=\"images/access_token.png\" />\n",
    "\n",
    "If you are going to put your code online anywhere, you should set environment variables for all four of these values; pasting them directly in your code is likely to result in you accidentally committing them and pushing them into your online repo at some point.  If someone gets a hold of these values they can access Twitter's API as you, which you don't ever want.\n",
    "\n",
    "Instructions for setting environment variables in an Anaconda virtual environment are [here](https://conda.io/docs/user-guide/tasks/build-packages/environment-variables.html).\n",
    "\n",
    "The code below accesses the virtual environment variables I've created to represent my consumer keys and access tokens and saves them as a variable in my code.  If you copy this workbook and run it locally on your computer, change the variable names in the quotes to the names you've given them in your virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_CONS_KEY = os.environ.get('T_CONS_')\n",
    "TWITTER_CONS_SEC = os.environ.get('T_CONS_SECRET')\n",
    "TWITTER_ACCESS_TOKEN = os.environ.get('T_ACCESS_')\n",
    "TWITTER_ACCESS_SEC = os.environ.get('T_ACCESS_SECRET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's done, we're going to use the python-twitter library to access the Twitter API.  I find this a lot easier than writing them from scratch. \n",
    "\n",
    "We imported the python-twitter libary as `import twitter` above.  It shows up in the [requirements.txt](requirements.txt) file as `python-twitter==3.4.2`, and you can read the documentation [here]\n",
    "(https://python-twitter.readthedocs.io/en/latest/), which also happens to have a great step by step on getting started with the Twitter API!\n",
    "\n",
    "The first thing will do is create an object that will hold all of the credentials for accessing the API that we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = twitter.Api(\n",
    "    consumer_key = TWITTER_CONS_KEY,\n",
    "    consumer_secret = TWITTER_CONS_SEC,\n",
    "    access_token_key = TWITTER_ACCESS_TOKEN, \n",
    "    access_token_secret = TWITTER_ACCESS_SEC,\n",
    "    tweet_mode='extended' # this ensures that we get the full text of the users' original tweets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!  You're ready to talk to the Twitter API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get some Tweets! üì¶\n",
    "\n",
    "This is where things start getting fun.\n",
    "\n",
    "If you haven't already, choose an account you want to analyze.  \n",
    "\n",
    "Next, you'll need to create a variable to hold that account's Twitter handle.  Input this in the cell below without the '@' (e.g. Use NASA, not @NASA).  If you're starting with the URL of the account, take only the handle after the last '/' (e.g. From https://twitter.com/NASA, use 'NASA'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_name = \"NASA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have that, let's get some data! We'll call the Twitter API with the `GetUserTimeline` method on our `t` object that we created in the last step.  \n",
    "\n",
    "Notice that we are passing in the `screen_name` variable, and setting the `count` variable to `200`.  You must have the screen_name to make the API call and if you omit the count variable, you'll get the latest 20 tweets back by default.  You can't get more than 200 tweets back at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Twitter API using the python-twitter library\n",
    "first_200 = t.GetUserTimeline(screen_name=screen_name, count=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 200 tweets in our dataset, which has the type <class 'list'>.\n",
      "\n",
      "Here's a sample of what the data from one tweet looks like:\n",
      "{\"created_at\": \"Sun Jul 01 23:04:00 +0000 2018\", \"full_text\": \"RT @NASA_Johnson: This week on \\\"Houston, We Have a Podcast,\\\" Harry Roberts, Flight Operation Supervisor for the Aircraft Operations Divisio\\u2026\", \"hashtags\": [], \"id\": 1013558869728071685, \"id_str\": \"1013558869728071685\", \"lang\": \"en\", \"retweet_count\": 96, \"retweeted_status\": {\"created_at\": \"Sun Jul 01 16:47:15 +0000 2018\", \"favorite_count\": 511, \"full_text\": \"This week on \\\"Houston, We Have a Podcast,\\\" Harry Roberts, Flight Operation Supervisor for the Aircraft Operations Division, talks about operations out at Ellington Field and the aircraft that helped make human spaceflight possible. https://t.co/nGdx8Lck02 https://t.co/L15ESgugoE\", \"hashtags\": [], \"id\": 1013464059197493249, \"id_str\": \"1013464059197493249\", \"lang\": \"en\", \"media\": [{\"display_url\": \"pic.twitter.com/L15ESgugoE\", \"expanded_url\": \"https://twitter.com/NASA_Johnson/status/1013464059197493249/photo/1\", \"id\": 1013463571764633602, \"media_url\": \"http://pbs.twimg.com/media/DhCLv7vUYAIu0iX.jpg\", \"media_url_https\": \"https://pbs.twimg.com/media/DhCLv7vUYAIu0iX.jpg\", \"sizes\": {\"large\": {\"h\": 1080, \"resize\": \"fit\", \"w\": 1080}, \"medium\": {\"h\": 1080, \"resize\": \"fit\", \"w\": 1080}, \"small\": {\"h\": 680, \"resize\": \"fit\", \"w\": 680}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"type\": \"photo\", \"url\": \"https://t.co/L15ESgugoE\"}], \"retweet_count\": 96, \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\", \"urls\": [{\"expanded_url\": \"https://www.nasa.gov/johnson/HWHAP/airspace\", \"url\": \"https://t.co/nGdx8Lck02\"}], \"user\": {\"created_at\": \"Tue Jun 23 21:54:57 +0000 2009\", \"description\": \"NASA's JSC is the lead center for the International Space Station and the Orion spacecraft, and the home of the Mission Control Center and NASA astronaut corps.\", \"favourites_count\": 2112, \"followers_count\": 819247, \"friends_count\": 172, \"geo_enabled\": true, \"id\": 50115087, \"id_str\": \"50115087\", \"lang\": \"en\", \"listed_count\": 8189, \"location\": \"Houston, TX\", \"name\": \"Johnson Space Center\", \"profile_background_color\": \"040A0A\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/50115087/1519655315\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/915969030250405888/T0OggIEx_normal.jpg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/915969030250405888/T0OggIEx_normal.jpg\", \"profile_link_color\": \"2E77B0\", \"profile_sidebar_border_color\": \"0E0F0D\", \"profile_sidebar_fill_color\": \"292929\", \"profile_text_color\": \"8A8A8C\", \"profile_use_background_image\": true, \"screen_name\": \"NASA_Johnson\", \"statuses_count\": 11275, \"url\": \"https://t.co/RZLaq2a7UQ\", \"verified\": true}, \"user_mentions\": []}, \"source\": \"<a href=\\\"https://www.sprinklr.com\\\" rel=\\\"nofollow\\\">Sprinklr</a>\", \"urls\": [], \"user\": {\"created_at\": \"Wed Dec 19 20:20:32 +0000 2007\", \"description\": \"Explore the universe and discover our home planet with @NASA. We usually post in EST (UTC-5)\", \"favourites_count\": 3612, \"followers_count\": 29625145, \"friends_count\": 286, \"id\": 11348282, \"id_str\": \"11348282\", \"lang\": \"en\", \"listed_count\": 91297, \"name\": \"NASA\", \"profile_background_color\": \"000000\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/11348282/1529640902\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/188302352/nasalogo_twitter_normal.jpg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/188302352/nasalogo_twitter_normal.jpg\", \"profile_link_color\": \"205BA7\", \"profile_sidebar_border_color\": \"000000\", \"profile_sidebar_fill_color\": \"F3F2F2\", \"profile_text_color\": \"000000\", \"profile_use_background_image\": true, \"screen_name\": \"NASA\", \"statuses_count\": 52268, \"url\": \"https://t.co/TcEE6NS8nD\", \"verified\": true}, \"user_mentions\": [{\"id\": 50115087, \"id_str\": \"50115087\", \"name\": \"Johnson Space Center\", \"screen_name\": \"NASA_Johnson\"}]}\n",
      "\n",
      "Each tweet is stored in a <class 'twitter.models.Status'> data structure.\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the data!\n",
    "print(\"There are %d tweets in our dataset, which has the type %s.\" % (len(first_200), type(first_200)))\n",
    "print()\n",
    "print(\"Here's a sample of what the data from one tweet looks like:\")\n",
    "print(first_200[0])\n",
    "print()\n",
    "print(\"Each tweet is stored in a %s data structure.\" % type(first_200[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see we got the first 200 tweets as a list of objects, and that there's a LOT of data in each one.  We'll come back to that in a moment, but first, let's get some more sweet, sweet data!\n",
    "\n",
    "The function below will help us get more data for our dataset.  The limitation we're working with, at the time of this writing, is that the Twitter API is rate limited, meaning Twitter puts restrictions on how much data you can take at a time.  For my app, I'm rate limited to 900 requests every 15 minutes.  \n",
    "\n",
    "Here's what the `get_tweets` function, below does:\n",
    "1. Takes the `first_200`, and `screen_name` variables as arguments, as well as something called `last_id`.  As you'll see when we call the function, this is the ID number of the last/oldest tweet in the `first_200` list.\n",
    "2. Makes a new list and adds the `first_200` tweets to it\n",
    "3. For 900 iterations (because of my rate limit):\n",
    "   - Gets 200 of the user's tweets, starting with a `max_id` one smaller than `last_id` in the previous list of tweets; the `new` variable this data is stored in gets overwritten each time\n",
    "   - Adds the list of tweet objects obtained to the `all_tweets` list\n",
    "   - If there's anything in the list (e.g. if we got any data back), grab a new `last_id` value to feed into the API call in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(first_200, screen_name, last_id):\n",
    "    all_tweets = []\n",
    "    all_tweets.extend(first_200)\n",
    "    for i in range(900):\n",
    "        new = t.GetUserTimeline(screen_name=screen_name, max_id=last_id-1)\n",
    "        if len(new) > 0:\n",
    "            all_tweets.extend(new)\n",
    "            last_id = new[-1].id\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/question.png\" align=\"left\">  <b>Why is max_id set to last_id MINUS 1?</b>\n",
    "\n",
    "In the 4th line of the `get_tweets` function, the `max_id` is set to `last_id-1`.  Why not just set it to `last_id`?\n",
    "\n",
    "Each tweet has a unique whole number, stored as the `ID` attribute; in the sample printed above it looks like this: `\"id\": 1013529203990581249`.   If we set the `max_id` to that number, the first tweet we get in the next 200 tweets will be that exact tweet, meaning it will appear twice in the dataset.  By subtracting one from this ID, we are saying the max ID can be anything less than the last tweet in our previous list of 200, which is exactly what we want. üëè\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run our function!\n",
    "all_tweets = get_tweets(first_200, screen_name, first_200[-1].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3245 tweets stored in a list as the all_tweets variable.\n",
      "The most recent tweet in our collection was sent Sun Jul 01 23:04:00 +0000 2018 and the oldest tweet was sent Wed Oct 11 15:31:37 +0000 2017.\n"
     ]
    }
   ],
   "source": [
    "# now to check the data - we should have more than 200 tweets.\n",
    "print(\"There are %d tweets stored in a list as the all_tweets variable.\" % len(all_tweets))\n",
    "print(\"The most recent tweet in our collection was sent %s and the oldest tweet was sent %s.\" % (\n",
    "                                                                            all_tweets[0].created_at, \n",
    "                                                                            all_tweets[-1].created_at)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Clean the data! üõÅ\n",
    "\n",
    "In this step, we're going to condense and clean our data to get it into a more analysis-friendly format.\n",
    "\n",
    "First, the condensing part.  We need to decide what's important for the analysis.\n",
    "\n",
    "Let's take another look at the data that comes back for with each tweet from the Twitter API.  There's a lot here, much of it not specific to the tweet itself.  For example, since we're grabbing all of the tweets from a single person's timeline, the whole `\"user\"` attribute isn't very useful to us, as it's repeated every time.  \n",
    "\n",
    "In this tutorial we'll focus on keeping and cleaning the following attributes, but you can choose your own and modify the code:\n",
    "- `id`: the unique identifier for the tweet\n",
    "- `created_at`: when the tweet was sent\n",
    "- `full_text`: the text included in the tweet\n",
    "- `hashtags`: the hashtags (e.g. \"#space\" appears as \"space\") included in the tweet\n",
    "- `urls`: the expanded version of urls included in the tweet (e.g. \"https://t.co/sYCFHKxzBf\" is the shortened URL in the tweet but we'll get the full url of what it points to, https://twitter.com/NASA/status/1013529203990581249/video/1)\n",
    "- `favorite_count`: number of times the tweet was favorited\n",
    "- `retweet_count`: number of times the tweet was retweeted\n",
    "- `source`: from what platform/app the tweet was posted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Sun Jul 01 23:04:00 +0000 2018\", \"full_text\": \"RT @NASA_Johnson: This week on \\\"Houston, We Have a Podcast,\\\" Harry Roberts, Flight Operation Supervisor for the Aircraft Operations Divisio\\u2026\", \"hashtags\": [], \"id\": 1013558869728071685, \"id_str\": \"1013558869728071685\", \"lang\": \"en\", \"retweet_count\": 96, \"retweeted_status\": {\"created_at\": \"Sun Jul 01 16:47:15 +0000 2018\", \"favorite_count\": 511, \"full_text\": \"This week on \\\"Houston, We Have a Podcast,\\\" Harry Roberts, Flight Operation Supervisor for the Aircraft Operations Division, talks about operations out at Ellington Field and the aircraft that helped make human spaceflight possible. https://t.co/nGdx8Lck02 https://t.co/L15ESgugoE\", \"hashtags\": [], \"id\": 1013464059197493249, \"id_str\": \"1013464059197493249\", \"lang\": \"en\", \"media\": [{\"display_url\": \"pic.twitter.com/L15ESgugoE\", \"expanded_url\": \"https://twitter.com/NASA_Johnson/status/1013464059197493249/photo/1\", \"id\": 1013463571764633602, \"media_url\": \"http://pbs.twimg.com/media/DhCLv7vUYAIu0iX.jpg\", \"media_url_https\": \"https://pbs.twimg.com/media/DhCLv7vUYAIu0iX.jpg\", \"sizes\": {\"large\": {\"h\": 1080, \"resize\": \"fit\", \"w\": 1080}, \"medium\": {\"h\": 1080, \"resize\": \"fit\", \"w\": 1080}, \"small\": {\"h\": 680, \"resize\": \"fit\", \"w\": 680}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"type\": \"photo\", \"url\": \"https://t.co/L15ESgugoE\"}], \"retweet_count\": 96, \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\", \"urls\": [{\"expanded_url\": \"https://www.nasa.gov/johnson/HWHAP/airspace\", \"url\": \"https://t.co/nGdx8Lck02\"}], \"user\": {\"created_at\": \"Tue Jun 23 21:54:57 +0000 2009\", \"description\": \"NASA's JSC is the lead center for the International Space Station and the Orion spacecraft, and the home of the Mission Control Center and NASA astronaut corps.\", \"favourites_count\": 2112, \"followers_count\": 819247, \"friends_count\": 172, \"geo_enabled\": true, \"id\": 50115087, \"id_str\": \"50115087\", \"lang\": \"en\", \"listed_count\": 8189, \"location\": \"Houston, TX\", \"name\": \"Johnson Space Center\", \"profile_background_color\": \"040A0A\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/50115087/1519655315\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/915969030250405888/T0OggIEx_normal.jpg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/915969030250405888/T0OggIEx_normal.jpg\", \"profile_link_color\": \"2E77B0\", \"profile_sidebar_border_color\": \"0E0F0D\", \"profile_sidebar_fill_color\": \"292929\", \"profile_text_color\": \"8A8A8C\", \"profile_use_background_image\": true, \"screen_name\": \"NASA_Johnson\", \"statuses_count\": 11275, \"url\": \"https://t.co/RZLaq2a7UQ\", \"verified\": true}, \"user_mentions\": []}, \"source\": \"<a href=\\\"https://www.sprinklr.com\\\" rel=\\\"nofollow\\\">Sprinklr</a>\", \"urls\": [], \"user\": {\"created_at\": \"Wed Dec 19 20:20:32 +0000 2007\", \"description\": \"Explore the universe and discover our home planet with @NASA. We usually post in EST (UTC-5)\", \"favourites_count\": 3612, \"followers_count\": 29625145, \"friends_count\": 286, \"id\": 11348282, \"id_str\": \"11348282\", \"lang\": \"en\", \"listed_count\": 91297, \"name\": \"NASA\", \"profile_background_color\": \"000000\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/11348282/1529640902\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/188302352/nasalogo_twitter_normal.jpg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/188302352/nasalogo_twitter_normal.jpg\", \"profile_link_color\": \"205BA7\", \"profile_sidebar_border_color\": \"000000\", \"profile_sidebar_fill_color\": \"F3F2F2\", \"profile_text_color\": \"000000\", \"profile_use_background_image\": true, \"screen_name\": \"NASA\", \"statuses_count\": 52268, \"url\": \"https://t.co/TcEE6NS8nD\", \"verified\": true}, \"user_mentions\": [{\"id\": 50115087, \"id_str\": \"50115087\", \"name\": \"Johnson Space Center\", \"screen_name\": \"NASA_Johnson\"}]}\n"
     ]
    }
   ],
   "source": [
    "print(all_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem is that the data we get back isn't totally clean, so we need to process it a little bit first.  Here are some examples from the data above that could create problems for us later because they include formatting we don't need.  In every case except the `created_at` attribute, we want a string, or list of strings, with just the important parts; we don't need `<a href=\"https://www.sprinklr.com\" rel=\"nofollow\">Sprinklr</a>`, just `Sprinklr`.\n",
    "\n",
    "For the `created_at` attribute, when using it in Python, we'll want to convert it to a datetime object.\n",
    "\n",
    "Let's take a look at some fields that need cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in the created_at attribute looks like this: Sun Jul 01 23:04:00 +0000 2018\n",
      "Data in the hashtags attribute looks like this: []\n",
      "Data in the urls attribute looks like this: []\n",
      "Data in the source attribute looks like this: <a href=\"https://www.sprinklr.com\" rel=\"nofollow\">Sprinklr</a>\n"
     ]
    }
   ],
   "source": [
    "print(\"Data in the created_at attribute looks like this:\", all_tweets[0].created_at)\n",
    "print(\"Data in the hashtags attribute looks like this:\", all_tweets[0].hashtags)\n",
    "print(\"Data in the urls attribute looks like this:\", all_tweets[0].urls)\n",
    "print(\"Data in the source attribute looks like this:\", all_tweets[0].source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get these fields into an easier to use format, I've written some helper functions, each one explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtags(hashtags):\n",
    "    \"\"\"\n",
    "    Turns data with any number of hashtags like this - [Hashtag(Text='STEMonStation')] - to a list like this -\n",
    "    ['STEMonStation']\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    if len(hashtags) >= 1:\n",
    "        for i in range(len(hashtags)):\n",
    "            cleaned.append(hashtags[i].text)        \n",
    "    return cleaned\n",
    "\n",
    "def clean_urls(urls):\n",
    "    \"\"\"\n",
    "    Turns data with any number of expanded urls like this - \n",
    "    [URL(URL=https://t.co/sYCFHKxzBf, ExpandedURL=https://youtu.be/34bFgA3H3hQ)]- to a list like this - \n",
    "    [\"https://youtu.be/34bFgA3H3hQ\"]\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    if len(urls) >= 1:\n",
    "        for i in range(len(urls)):\n",
    "            cleaned.append(urls[i].expanded_url)\n",
    "    return(cleaned)\n",
    "        \n",
    "\n",
    "def clean_source(source):\n",
    "    \"\"\"\n",
    "    Turns data including the source and some html like this - \n",
    "    <a href=\"https://www.sprinklr.com\" rel=\"nofollow\">Sprinklr</a> - to a list like this -\n",
    "    ['Sprinklr']\n",
    "    \"\"\"\n",
    "    raw = lxml.html.document_fromstring(source)\n",
    "    return raw.cssselect('body')[0].text_content()\n",
    "\n",
    "\n",
    "def string_to_datetime(date_str):\n",
    "    \"\"\"\n",
    "    Turns a string including date and time like this - Sun Jul 01 21:06:07 +0000 2018 - to a Python datetime object\n",
    "    like this - datetime.datetime(2018, 7, 1, 21, 6, 7, tzinfo=datetime.timezone.utc)\n",
    "    \"\"\"\n",
    "    return datetime.strptime(date_str, '%a %b %d %H:%M:%S %z %Y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use all of these helper functions in the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Reformat the data as a csv or Python dictionary üóÉ\n",
    "This is the last step of this tutorial, where I'll show you how to get this data, which we now know how to retrieve and clean, into a format that we need to start the analysis.\n",
    "\n",
    "I'm showing both how to make this into a csv and a Python dictionary because:\n",
    "- A lot of people like seeing the data all at once as a csv - there are neat ways to print it in Python but this is easier to absorb for a lot of people\n",
    "- csv is a great format to use if you need to share the data with people who don't program, as they can open it up in any notepad or spreadsheet program, no coding required\n",
    "- If you want to continue the analysis with Python but in a different project file, you can always use the dictionary or read in a csv in a couple of lines of code\n",
    "\n",
    "So let's start with the csv file! The `write_to_csv` function will create the file and pull the data we want out of the `all_tweets` list of tweet objects we made in step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(tweets, filename):\n",
    "    # the headers are the fields that we identified in step 4\n",
    "    headers = ['id', 'full_text', 'hashtags', 'urls', 'created_at', 'favorite_count', 'retweet_count', 'source']\n",
    "    \n",
    "    # here we create the file and write the header row with the headers list\n",
    "    # note that the 'filename' argument will be the name of the csv file\n",
    "    with open(filename + '.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        # in this loop, we write a new row for each tweet object, with the data taken from the tweet object in \n",
    "        # the order we listed the headers\n",
    "        # note where we call the helper functions from step 4 on hashtags, urls, and source\n",
    "        for item in tweets:\n",
    "            writer.writerow([item.id, \n",
    "                             item.full_text, \n",
    "                             clean_hashtags(item.hashtags), \n",
    "                             clean_urls(item.urls), \n",
    "                             item.created_at, \n",
    "                             item.favorite_count, \n",
    "                             item.retweet_count, \n",
    "                             clean_source(item.source)])\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we call the function, passing in the all_tweets list\n",
    "# here the filename will be the screen_name variable defined in step 2 with \"_tweets\" after it (e.g. NASA_tweets.csv),\n",
    "# but you can change it to whatever you want\n",
    "write_to_csv(all_tweets, screen_name + '_tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should see your csv file in the same directory as this notebook!  Check that it has all of the right fields and the right number of rows; there should be one for every tweet in the list plus a header row.  \n",
    "\n",
    "You can see the file, current as of July 1, 2018, on Google Drive [here](https://docs.google.com/spreadsheets/d/1CaYfh9xJgJRPiN-bYn_ewaMCkp1uNkC1kC7E6pgy1lg/edit?usp=sharing), as well as in this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're more intersted in continuing to analyze the file in Python, I'd recommend putting the data in a dictionary. Some of this is pretty similar to the process for writing the data to a csv file, although notice that we need to put each one under a key (here I've chosen the string version of the ID field).  Notice I'm running all four of the cleaning functions from step 4 on the data as I add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(tweets):\n",
    "    dict = {}\n",
    "    for item in tweets:\n",
    "        clean_source(item.source)\n",
    "        dict[item.id_str] = {\n",
    "            'full_text': item.full_text,\n",
    "            'hashtags': clean_hashtags(item.hashtags),\n",
    "            'urls': clean_urls(item.urls),\n",
    "            'created_at': string_to_datetime(item.created_at),\n",
    "            'favorite_count': item.favorite_count,\n",
    "            'retweet_count' : item.retweet_count,\n",
    "            'source': clean_source(item.source)\n",
    "        }\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict = create_dict(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full_text': \"In this week's #STEMonStation, @astro_ricky demonstrates how water's molecular properties behave in microgravity and the unique opportunities it creates on @Space_Station: https://t.co/sYCFHKxzBf https://t.co/KZrx0H5HNH\",\n",
       " 'hashtags': ['STEMonStation'],\n",
       " 'urls': ['https://youtu.be/34bFgA3H3hQ'],\n",
       " 'created_at': datetime.datetime(2018, 7, 1, 21, 6, 7, tzinfo=datetime.timezone.utc),\n",
       " 'favorite_count': 1587,\n",
       " 'retweet_count': 429,\n",
       " 'source': 'Sprinklr'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_dict[\"1013529203990581249\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this tutorial!  Stay tuned for Part 2 in [this repo].(https://github.com/eleanorstrib/twitter_timeline_analysis_2) and on [agatha.codes](https://medium.com/agatha-codes) very soon."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
